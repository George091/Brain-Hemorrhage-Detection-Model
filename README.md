# Brain-Hemorrhage-Detection-Model

This is the readme for the Intracranial Hemorrhage (IH) detection model by George Barker, Julio Hidalgo Lopez, and Andre Zeromski. This model uses a convolutional neural network (CNN) to identify brain hemorrhaging and to determine what type of hemorrhaging is present in a CT brain scan. The dataset we used is part of the Kaggle RSNA Intracranial Hemorrhage Detection competition provided by the Radiological Society of North America. We are using data from phase 2 of the competition, which consists of about 120,000 test images and 752,000 training images. We also have a CSV which contains information about the type of hemorrhage present in each scan of the training images. Each of the images are in the form of DICOM files with pixel array dimensions of 512x512. From these images, we plan on training an AI how to detect the presence of brain hemorrhaging and to identify the type of hemorrhaging. The DICOM files provided by the competition are CT-Scans that also come with a range of X-Ray information much more extensive than what we need to train our (IH) indentification model. The DICOM files' metadata is useful for our preprocessing phase. The metadata we use in preprocessing include "Rescale Intercept" and "Rescale Slope".

CT-Scans contain raw information on Hounsfield units (HU) obtained during the scanning process. HUs are a measure of relative radio-intensity, which essentially capture the density of the material the X-Rays hit as they go across the patient. Water’s density is considered 0 on this scale, with less dense materials returning negative HU values and more dense materials returning positive HU values. The range of HU values gathered during a CT scan can range from -1000 to 4000 HU depending on where in the body the scan was taken. Clearly, this range is too large to accurately depict using the standard range of 0 to 255 for pixel values. Further, the human eye can only differentiate about 17 different shades of gray, so analyzing a image with 255 distinct shades of grey would be impossible even to the trained eye. The medical profession gets around this problem by defining a window of HU values to display based on the body region of interest and the density of the features needed to access for diagnosis. This allows medical professionals to see important details that they would not be able to discern if the entire spectrum of raw HU values were converted to greyscale. In the cranial region, there are four such windows that are relevant for diagnosis by a physician: the brain window, the vascular window, the subdural window, and the bone window. The subdural, vascular, and brain windows are quite close in terms of the range of values they encompass, but small distinctions between them make them the most relevant windows for diagnosing IH. The brain window displays the soft gray and white matter tissues of the brain very well, and the subdural window displays the meninges and acute blood clots, while the vascular window is very good for seeing pooling blood.

We take a similar approach when restricting our window size to provide useful data to our CNN. We standardize all our DICOM images to display a vascular window on one channel, a brain window on a second channel, and a subdural window on a third channel. Therefore, the model’s input are tensors with dimensions 512x512x3. We choose to use the vascular window as it is easiest for a human to spot an IH in this window due to its narrow range accentuating minor differences in densities. This window has a range of 90 HU units centered around 68 HU and allows blood to be seen very clearly in contrast to brain and bone. The brain window will yield information on the state and morphology of the soft grey and white matter of the brain, which may be altered due to an IH. This window has a range of 70 HU centered around 40 HU. We decided to use a subdural window, which is not usually used by radiologists, but it is widely used in the AI literature covering this problem and could help differentiate the subdural and epidural sub-types of IH. The subdural window used in the course of our research was set to 200 HU centered about 80 HU.

 In the future, we may also implement a technique that we have come across called "sigmoid windowing" where HU values in the window are passed through the sigmoid function (making sure to preserve their relationship but fit within the (0, 1) range of the sigmoid function) in order to add an extra level of contrast between the central HU values of the window.  This could help further point out features of interest within the image: HU values closer to the center of the window will be more contrasted, while the more extreme values in the window will be less contrasted. However, we wish to test the accuracy of our CNN on arrays composed of vanilla windows first. We are also considering adding a fourth channel containing a bone window to provide the model with more extractable features within our data. This window would have a range of 2000 HU centered about 600 HU.

Following our processing, we are left with 512x512x3 arrays that we can feed to our neural network. All the values within this array are within 0 and 1 which will help keep the value of the nodes from exploding as they propagate through. Each image is individually labeled with a unique ID and what type of IH is present, if any, so the CNN will be able to compute a loss function correctly at the end of each iteration. The first layer of our model will be an inception layer, and we plan on using a 5x5x3 kernel in our two inter convolution layers, both of which will be followed by max pooling layers. The output of the third and final convolution layer in our architecture will be fed to a fully connected layer  and softmax layer for image classification. 

To provide a detailed account of our code implementation for the data preprocessing section:

We define a function for making our windows called "window_image". It takes a DICOM image, the desired widow range, and the desired window center as arguments; and returns an array of HU values within that window. Internally, it extracts the scale intercept and scale slope values from the DICOM's metadata and then multiplies all the values in its pixel array by the scale and adds the intercept. This is done to standardize all our DICOM images as they may have been taken on different machines and with different settings. We then make our window by taking all the HU values in the original DICOM image that are below our window range and set them to the value of the lower window bound. Likewise we set all HU values above our window range to the value of the upper window bound. Lastly, we simultaneously make the values within the window contrast more with each other and set the values of the new array to between 0 and 1 by subtracting the lower bound from each element and then dividing the result by the difference between the largest and smallest values in the array. 

We then define a second function called "composite_image" to combine our desired windows into one final 512x512x3 numpy array that is returned and can later be fed into our CNN. This function also takes a DICOM file and makes a brain window, vascular window, and subdural window by passing that DICOM file through our "window_image" function with a range and center of (70, 40), (90, 68), and (200, 80) respectively. We then make an empty numpy array of shape 512x512x3 and place each of our windows into one of the three depth locations, or channels. Lastly, we return this final array which we can feed to our CNN.

See the attached image, filename is "BeforeAndAfterProcessing.png" to see the results of our preprocessing stage for 5 images.
