# Brain Hemorrhage Detection Model

- [How to Run Code](#How-to-Run-Code)
- [Imports](##Imports)
- [window_image Function](##window_image-Function)
- [composite_image Function](##composite_image-Function)
- [create_partition_and_labels Function](##create_partition_and_labels-Function)
- [DataGenerator Class](##DataGenerator-Class)
- [Code Flow](##Code-Flow)
- [Basic Vanilla CNN](##Basic-Vanilla-CNN)
- [AlexNet](##AlexNet)
- [Inception V3](##Inception-V3)
- [DenseNet](##DenseNet)
- [ResNet](##ResNet)
- [Loading and Saving](##Loading-and-Saving)
- [Evaluation](##Evaluation)
- [Trouble-shooting](##Trouble-shooting)
  * [Corrupted data: On the hard drive and from Kaggle](###Corrupted data:-On-the-hard-drive-and-from-Kaggle)
  * [Loss Function: Categorical vs binary cross entropy](###Loss-Function:-Categorical-vs-binary-cross-entropy)

## How to Run Code
Download all files and data from the provided github link: https://github.com/George091/Brain-Hemorrhage-Detection-Model. Run the “Test-Model.py” file after downloading the entire repository. This python file shows the following in the console: (1) an example of our model’s predictions on a positive case (brain hemorrhaging) (2) an example of our model’s predictions on a negative case (no brain hemorrhaging) (3) our model uses the data generator to train a model using fit_generator on a subset of the whole dataset (4) our model uses the data generator to evaluate a model using evaluate_generator on a subset of the whole dataset. It is important to note, that this model pickles in the partition and labels dictionary described in the section on create_partition_and_labels function. The excel file containing the labels was over 100 MB and too large for github to contain, so we serialized the dictionaries for a subset of the data. 

We also included a file Inception-Model.py that will not run as is, but is available to see the logic of our overall framework on how we train, evaluate, and save a model. This includes cleaning data and feeding data to the model. We describe this script in the below README. Inception-Model.py can be run in a Kaggle kernel on kaggle.com after importing the RSNA dataset into the input of a kernel.

## Imports
We begin by importing modules we will need to process our data and build and train our model. Amongst these are numpy, keras, and pandas. One module worth noting is the pydicom module. This is a model that was developed to read DICOM files: a file type used by medical imaging technology including Computerized Topography software. We need this module in order to read and work with our DICOM datasets. 

## window_image function
The DICOM files that we are working with contain CT images composed of a range of Hounsfield Units (HU), a measure of radio-intensity, that range from [-1000, 1000] based on the density of the material the CT machine’s X-Rays encounter when passing through the body. The density of water is considered 0 on this scale. Further, based on the settings and CT machine model used, the HU values on any two CT scans may differ from each other. Therefore, the first thing this function does is linearize the HU values of a given CT scan by extracting its slope and intercept from its metadata and multiplying the whole pixel array by the slope and adding then adding the intercept to each pixel in the pixel array. This standardizes our data and will allow the model to extract features in the scans relevant to the region of interest in the search space. 
Secondly, given the wide range of HU values covered in a CT scan, there is no way that they could all be represented within the pixel value range of [0, 255]. Additionally, Even when dividing the HU range by 255 and fitting intervals of 8 HU into each grayscale pixel value, we would lose important features essential for diagnosis as many features, especially in soft tissues such as the brain, lay within very close HU values. Radiological experts circumvent this problem by the practice of “windowing”. Here, we only see HU values within a “window” range centered around a “level” HU value. HU values above this window range saturate the pixel (set it to 255) and HU values below this window turn the pixel off (set it to 0). By changing the window size and level we can extract important features from different tissue types. For example, we can define a “brain window” to see features of the brain and similar density tissues, such as blood, very well. 
The window_image function requires as parameters a DICOM file, a window size, and a level to convert the image to the specified window (brain, subdural, vascular, or bone). After standardizing the pixel array, we create the desired window by setting all pixel values above the level plus half of the window size to 255 and setting all pixel values below the level munis half of the window size to 0. We then evenly spread the pixel values within our window range over the pixel range by setting each pixel value in the image to (the pixel value minus the minimum pixel value in the array) divided by (the maximum pixel value minus the minimum pixel value). The function then returns this standardized, windowed, pixel array. 

## composite_image Function
This function utilizes the window_image function to make three windowed pixel arrays from a passed DICOM image file argument. It then combines these arrays into a single image by making a numpy array of size 512, 512, 3 with each of the windowed pixel arrays comprising one of the depth layers. It then returns this array. Here we combine a brain window (window: 40, level: 40), a vascular window (window: 90, level: 68), and a subdural window (window: 200, level: 80). To make sure that all our outputs from this function have the same dimensionality, we pad any output image to a width and height of 512 if it is not already that large. Due to one corrupted file in the dataset, we built a fail-safe into this function by using a try-except block. We try the above code (which works for all non-corrupted images) and catch any ValueError (the corrupted image). This corrupted file is in the training set. The except section of this function prints an error statement and returns a 0s array of shape 512, 512, 3. Due to the exceptionally large nature of our data set and having just the one corrupted file, sending the 0s array through the model during a training run will have negligible effects on backpropagation and training. 

## create_partition_and_labels Function
This function partitions the dataset into a training set (internally set at 80% of the dataset) and a validation set (internally set at 20% of the dataset) and makes dictionaries that pair each training example and its list of labels (binary classifications; 5 hemorrhage subtypes and whether or not there is a hemorrhage or not. The subtypes are not mutually exclusive). It then returns a partition dictionary (with train and validation as keys for the entire respective lists of string IDs of patients in the respective partition) and the dictionary of labels. The function takes a data_location argument that tells it the directory where it can access the dataset. It accesses the .csv file for the dataset to  match all patient scans with their associated labels. It then begins to build the labels dictionary by matching each patient ID (each patient has several scans associated with this ID) to its diagnosis labels and image IDs. Then the function proceeds to separate the indexes of the list of DICOM images in the dataset into training and validation sets based on the pre-set split ratio. It then shuffles the training set to randomize the order of the training examples, only, so that the validation set is the same across models. Since the filename of each DICOM file is the patient ID, we will still be able to access the proper labels from the labels dictionary (since the patient ID is the key for the label). We then place the training and validation sets into a partition dictionary as lists under the labels ‘train’ and ‘validation’. Lastly, we return the partition and labels dictionaries as a tuple.

## DataGenerator Class
Due to the sheer size of our dataset, we cannot load our entire dataset at once for training our model since this would exhaust our RAM’s capacity. To get around this, we design a Data generator class which inherits from the keras Sequence object to use the built in keras methods. It builds smaller training batches that can be fed to the model without overloading the RAM. The data generator instance is later passed as an argument to the model’s fit_generator function. The class itself is composed of an __init__ method, a __len__ method, and a __getitem__ method. The __init__ method takes an ID list (the training or validation partition taken from the dictionary made by the create_partition_and_labels function), a labels dictionary (contains all the patient IDs as a key and the y_actual as the value), and a desired batch size for the generator. It then stores these arguments into internal instance variables. The __len__ method returns the number of batches per set by dividing the length of the ID list by the batch size and rounding up. This method can be used for training or for evaluation to retrieve the number of batches per epoch, such as the implementation of a loading bar. The __getitem__ method is called by the model’s fit_generator and evaluate_generator methods under the hood to feed batches to the model. It takes an index as an argument which is, likewise, passed by the fit_generator and evaluate_generator methods under the hood. It then makes a sublist of the ID list by splicing it between (the index times the batch size) to ((the index plus one) times the batch size). It then makes a sublist of the labels list by adding the labels associated with every item in the ID sublist to the labels sublist from the labels list. It then gets the HU images from the dataset by using each of the IDs in the ID sublist and passes each image through the composite_image function to transform them into tri-windowed pixel arrays. Lastly, the method converts both the sublist of tri-windowed pixel images and the sublist of associated labels into numpy arrays that it then returns. 

## Code Flow
After importing necessary modules and defining the above functions and DataGenerator class, we define the global constant: data_location, the root path that informs the program of where the dataset is located. It is currently set to the kaggle competition data repository where the RSNA dataset is stored. In desktop-only implementations this can be changed to the directory where the dataset is stored. 
We then call the create_partition_and_labels function to get a partitioned dataset dictionary with a training set of 80% of the dataset and a validation set of 20% of the dataset; we also get a  dictionary of the labels of the dataset. Due to training time constraints (limited GPU time running on kaggle, and only being able to access our flash drive with the dataset on one lab computer at a time) we forego making a batch generator for the competition testing dataset (which is a substantial size). Instead, we use the validation split of the training dataset as the testing set. We feel this is justified as the variability in human anatomy and the fact that we are typically only running one epoch for training should sufficiently curb overfitting; further, the size of the validation split (20% of 752,800) should be large enough to adequately gauge the performance of the model.
We then define the batch size to be 64 examples and make DataGenerators for the training and validation (testing) partitions. For deeper models, we reduce the batch size to prevent RAM from exhaustion. We pass the respective partitions from the partition dictionary and the labels dictionary to these objects.
Next we build our model architecture. We begin with a very basic vanilla CNN, then move on to three pre-trained image recognition models, including DenseNet, ResNet, InceptionV3, and a modified AlexNet architecture and compare the performances of each. We only built one model at a time using code environment copies and a plug-and-play approach. This way we could train and evaluate multiple models at once and obtain our data in a reasonable time frame. In the code turned in we included the model with the best performance after empirical testing. More information on each model architecture is included below.
We then compile the model with a binary cross entropy loss function and an Adam optimizer. We chose to use binary cross entropy as it matches our label data which consists of an array of six nodes comprised of 1 and 0 values which correspond to the presence or absence (respectively, of the categorization encoded by position in the array (The first index denotes wether or not there is an IH). Since the output node of the models consist of a 6 node dense layer activated by the sigmoid function, the values of each node will be within (0, 1) and we can use this to backpropagate error compared to the values of the label. We chose to use the sigmoid function because of its range, and not a softmax, because of the non-mutually-exclusive label values. We chose the Adam optimizer as it is an industry best practice and due to our long training time we did not have the ability to tune hyperparameters (the hyperparameters of all model architectures are also set to best practice values and not tuned for this reason). Lastly, we specify our metrics to be: accuracy, true positives, true negatives, false positives, false negatives, and Area Under the Curve. The true/false positives/negatives are calculated as the average number occurring per batch, so often they come out as floating point values for each batch. Using multiple metrics will make use better able to compare the performance of each of our models.
We then train the model using its fit_generator method with our training_generator, epochs = 1, and verbose = 2. The training_generator object will feed batches of training data one at a time to the model and will keep the computer from running out of RAM by loading in the entire training set at once. We chose to only run the model for one epoch due to the large size of the data set and time restrictions (even on GPU, the model takes several hours or even a day to train on one epoch based on its complexity); further, only running one epoch will help keep the model from overfitting the data. Setting the verbose setting to 2 will print out the state of the model at the end of training.
Following training, we save the model and its weights as .json and .h5 files respectively. This allows us to load in the model for implementation, demonstration, or further training later on.
Lastly, we evaluate the model and print out its performance metrics. We do this by calling the model’s evaluate_generator method, passing in our validation_generator (testing generator) as the generator, and setting verbose = 2 as for the training run. As with the fit_generator above, the evaluate_generator method will pass testing batches from the validation_generator to the model one at a time to evaluate the model without running out of RAM by loading the entire dataset at once. The verbose setting will show the performance of the model at the end of the testing run.

## Basic Vanilla CNN
The first model architecture we chose to test is a simple CNN implementation. It consists of a convolutional layer followed by a max-pooling layer, the output of which is flattened and mapped to a 6 node, sigmoid-activated, output node. To build the model, we call keras’s Sequential class and then add layers appropriately. 
The first layer is a keras Conv2D layer to which we pass a filter size of 32, kernel size of (5, 5), stride length of (2, 2), the relu activation function, and an input shape of (512, 512, 3) as arguments. This instantiates a convolution layer with a 5x5 kernel that traverses the input image with a stride of 2x2, meaning that it will jump a pixel as it moves along the image in the horizontal and vertical axes. At the end of the convolution it will return a convolved image with 32 filters representing 32 different features extracted from the input image. We utilize the relu activation function as it is the most widely used activation function for CNNs and it keeps data from being lost between convolution and pooling layers as it does not approach an asymptote for all positive pixel values (meaning that it never will as pixel values do not go below 0). We specify an input shape since this layer is the first in our architecture and therefore needs to know the size of the inputs it will get in order to internally set up its algorithms properly.
Our second layer is a keras MaxPooling2D layer to which we pass a pool size of (5, 5) and a stride length of (2, 2) as arguments. We chose these arguments to fit with those of the previous convolution layer. This way, the pooling makes sense as it will only highlight the most prominent features within every processed step of the convolution. This layer does not require an activation function as internally it only takes the largest value within the pooling kernel and converts the entire kernel to that value. 
Next, we flatten the output of the max pooling layer by adding keras’s Flatten layer which takes no arguments and returns a one dimensional hidden dense layer representation of the previous layer with a number of nodes equal to the volume of the previous layer. We do this so we can use the features from the convolution and pooling layers to reach a classification in the output layer, which is also one-dimensional. 
Lastly, We connect the nodes of this hidden layer to a fully-connected output layer by adding a keras Dense layer with 6 nodes and a sigmoid activation function as arguments. This will return an output layer of 6 nodes with values between (0, 1) due to the sigmoid function. This patches our label data which is an array of 6 nodes with values that are either 0 or 1 denoting the presence or absence of a category (encoded by position) which are not mutually exclusive. This will allow us to calculate a loss and train the weights of the model via back-propagation.
As mentioned earlier, we used hyperparameters that are best practices in industry and we have seen in other implementations as we did not have time to do a grid search to tune our hyperparameters to optimal values for our problem.

## AlexNet
The AlexNet is a modification of the original AlexNet architecture and our code is based on a python implementation cited below. The major differences are that the input shape is set to  (512, 512, 3) instead of (256, 256, 3) and that the output layer is a fully connected layer of 6 nodes with the sigmoid activation function (for the reasons mentioned previously). The architecture consists of five 2D convolution layers followed by three dense layers before the output layer. The first two convolution layers and the last convolution layer are followed by a max pooling layer. Dropout layers with a rate of 0.5 follow each of the above layers as well as the first two dense layers. All hyperparameters are the same as are used by the AlexNet architecture and, therefore, pre-tuned to a certain degree that we assume will still be useful for our larger input These are all explicitly defined in the code and may be found there. The first implementations we tried failed due to the filter size not being explicitly stated and using same padding as opposed to valid padding. The later caused dimensionality errors. Once we implemented a working architecture we ran into errors towards the end of training due to the batch size, so we reduced it from 64 to 32 to remedy the issue. We later  decreased the batch size to 8 when it appeared that a training run froze, but further analysis of training revealed that the model will still train properly with a batch size of 32.

## Inception V3
For this model, we import an InceptionV3 model with pretrained ImageNet weights. We use pretrained weights because InceptionV3 is composed primarily of convolution layers, and convolution layers are translationally invariant, so these tuned weight should extract features relevant for our purposes. The inception model is composed of inception layers. An inception layer applies separate kernels to the same base/input to the layer, and the results are concatenated in the output of the inception layer. For example, a typical inception layer takes its base/input, applies a 1x1 kernel then a 5x5 kernel to it. It then takes the original base but this time applies a 1x2 kernel followed by a 3x3 kernel. The inception layer does this again for a pooling layer then a 1x1, and just a 1x1. If the input/base is multi channeled, then the 1x1 kernel is effective in reducing dimensionality. The result of the layer is concatenated because, in this way, a group of unique features are extracted from the base: features requiring a 5x5 kernel were extracted, features requiring a 3x3 kernel were extracted, and so on. The goal of the InceptionV3 was to create a deep model that reduced overfitting by using sparsely connected network architectures that replace fully connected network architectures. Essentially, it replaces large convolutional layers with smaller ones. The idea is that a 5x5 convolutional layer can be replaced with two 3x3 convolutional layers, bringing the number of parameters down from 25 to 18. Additionally, 3x3 convolutions can be replaced by a 1x3 and 3x1 kernel, reducing parameters by 33%. It was found that these modifications do not drastically affect performance in the later layers, but greatly improve efficiency. Therefore, the InceptionV3 stands out from its predecessors by utilizing “factorizing convolutions”. Additionally, InceptionV3 utilizes “grid size reduction”, which is essentially dimensionality reduction by using a convolutional layer and pooling layer to reduce the base’s dimensionality by 75% each, and both are concatenated into a dimension that is 50% the dimensionality of the original base. The architecture of the InceptionV3 is three convolution layers followed by a maxpool, two more convolutional layers, a max pooling layer, followed by three inception layers, a grid size reduction block,  four inception layers, grid size reduction, two more inception layers, average pooling layer, dropout layer, dense layer, and a softmax output layer. Originally, we created the model so that all the layers were frozen, so the only weights that were trained were the ones coming out of the Inception V3’s softmax to the output layer. In our second model, we made it so that the dense layer, softmax and output were trainable. All in all, the convolution layers, inception layers, and pooling layers are used for maximal and robust feature extraction, grid size reduction for feature extraction and dimensionality reduction, and dropout at the end to reduce overfitting. 

## DenseNet
Our DenseNet model uses an instance of keras’s DenseNet model pretrained on the ImageSet dataset as the backbone of our architecture. The model starts with an input shape of (512, 512. 3) which feeds the data directly into an instance of keras’s DenseNet model. This model is comprised of dense blocks and transition layers. Dense blocks are consisted of a composition and bottleneck layer. Each composition layer uses batch normalization to normalize the value distribution in the layer by adjusting and scaling the layers activations. Batch normalization helps achieve higher accuracies and a faster training speed. In addition to batch normalization a relu activation function is used as is standard for a CNN. A 3x3 convolution is done to output a feature map. Inside of the dense block the composition layers are preceded by a bottleneck layer, which includes a 1x1 convolution used to map an input pixel and its channels to an output pixel in order to achieve dimension reductionality. The bottleneck layer utilizes batch normalization, and a relu activation function for mainly the same reasons as the composition layer. Inside every dense block each layer takes inputs from all preceding layers, in the same block, and passes on its feature map to all subsequent layers in a feed forward fashion. This is the main benefit of using a dense layer as it reduces the amount of parameters, promotes feature reuse, and strengthens feature propagation because it does not need to relearn redundant feature maps. Additionally this causes a more efficient flow of information. After every dense block is a transition layer that contains a 1x1 convolution followed by a 2x2 average pooling. After the final dense block a global average pooling is performed to minimize overfitting and achieve dimensionality reduction in a more extreme manner than max pooling by taking the average of all feature maps. This is connected to a final output layer of 6 nodes and a sigmoid activation function. We compiled this model using a binary loss entropy loss function and trained the model on 50,000 images in the training set with one epoch. We found to make an accurate densenet model did not require training on the entire training set.  

## ResNet
Our ResNet model, much like our DenseNet model, utilizes an instance of Keras’s 50 layer ResNet as a backbone for our model.  We first specify an input shape of (512, 512, 3) that feeds our data into keras’s ResNet. This ResNet is pretrained on the ImageSet dataset. This ResNet uses skip connections so that it will completely skip over layers generally at the beginning of training the model. Essentially the activation from one layer will be imputed to a deeper layer in the model than is expected. This is to avoid a vanishing gradient problem by reusing activations from a previous layer until the adjacent layer is able to learn its weights. More skip connections will be used earlier in training. This allows the ResNet to train quicker than most neural networks of the same depth. The model then gradually restores these skipped layers as it learns. The ResNet uses both batch normalization and a relu activation function in a similar manner to the dense net. This is all connected to a global average pooling layer to minimize overfitting and achieve dimensionality reduction in a more extreme manner than max pooling by taking the average of all feature maps. A global average pooling layers are often connected to ResNets, internally keras ResNet model utilizes a global average pooling layer to ensure the model is generalizable and to reduce the amount of parameters the model has. Consistent with all of our models this layer is connected to a final output layer of 6 nodes and a sigmoid activation function and uses binary cross entropy as a loss function. Similar to the DenseNet the ResNet was trained on 50,000 images. 

## Loading and Saving
Due to long training times, we decided it would be best to save our model architectures and associated weights after each training epoch. This way, we could load any model back into the kaggel environment after being away from the computer and continue further training runs if desired or begin testing. At first, we attempted to pickle and to use model.save however, when the file was loaded back into memory we received an unknown metric error, since we imported the TP,FP,TN,FN, and AUC metrics from tensorflow. Since we only cared about the architecture and weights anyways, we decided saving other details besides weights and architecture was unnecessary. The saving algorithm uses keras API calls that allow us to save the model architecture as a .json file and the weights as a .h5 file. The loading algorithm navigates down to the specified path were these files are stored and loads them by name into a new model, likewise using keras API calls. Since neural network models are deterministic, using the same architecture with the same weights will always yield the same result when faced with the same data. Therefore, saving and loading the model is a convenient way to access the model’s training and testing at any step, and to address emergent problems, without having to worry about inadvertently influencing the training process away from what it would be if one were to run the whole process at once.  

## Evaluation
We chose to use a variety of metrics to evaluate our model. We chose to track accuracy, true positives, true negatives, false positives, false negatives, and area under the curve. Accuracy is measured as the nodes in the output layer predicted correctly over the total number of nodes. So for 1 image, we could obtain an accuracy of 4/6 if the network correctly classifies that there is a bleed, but then incorrectly classifies a type. The true and false positives and negatives are evaluated in a similar way with each label for an image counting towards the total of positives and negatives. So the sum total of these 4 metrics will be the number of images analyzed times 6, since there’s essentially 6 predictions for each image. These values are normally decimals as keras tracks this as an average over the whole epoch during training or validation. Finally the area under the curve metric was used as well. Area Under the Curve is a more robust calculation of a model’s accuracy when it yields a probabilistic output. The accuracy metric has a threshold of 0.5 for these models with any value above considered positive and any value below considered negative. An AUC closer to 1 means the model is predicting more 1s and 0s correctly. However, the bimodal distribution of a model’s positives and negatives may not be centered around 0.5. For example, a model’s positive output may be 0.8 or higher and its negative output may be 0.6 or lower, in which case the accuracy metric would count them both as positives. AUC circumvents this problem as it considers the probability threshold of any given model, and thereby better counts it's true positives and true negatives.

## Trouble-shooting 
### Corrupted data: On the hard drive and from Kaggle
Throughout the course of the project, we encountered a few problems with our data. To begin, the dataset contained duplicates. We removed these in the create_partition_and_labels function, as they would increase overfitting. We also initially wanted to offload all of the data from Kaggle onto an external hard drive, just in case the data on Kaggle became unavailable. This process of downloading and unzipping took 48 hours, as our final dataset was about 1 terabyte. However, when we attempted to use this data, our models were encountering errors when attempting to obtain batches from the external hard drive. We learned that the data had been corrupted in the file transfer process, and we had to think of ways to re-access the data without wasting another 48 hours downloading it. We found out that the Kaggle website still had the data available on the cloud, so we just used the online kernel and cloud computational environment to run and test our models. However, we ran into another problem, when we learned that our dataset was not standardized in size. Most images were 512x512, however some were 640x640, and even some were corrupted and thus their pixel data was inaccessible. To combat this, we standardized each image to be 512x512, where smaller images were copied over in their entirety in the top left corner, and larger images were cropped to fit 512x512. All in all, we found that images larger and smaller than 512x512 were less than .01% of the total dataset, so we did not worry about losing vital information from them by manipulating them in this way. The single corrupted image was handled by applying a try-except in the preprocessing step, where if the image’s pixel data was inaccessible (resulting in a ValueError), the model skipped over that image, printed an error message, and continued with the rest of the images for training.

### Loss Function: Categorical vs binary cross entropy
In our project it was clear to us what we wanted the output layer to be. We wanted 6 nodes in the output layer, with the first node being whether there is any type of bleed in the image, and the other 5 nodes corresponding to a classification of the type of bleed. Our label looks like the following [0, 0, 0, 0, 0, 0] for the majority of our images, which don’t have bleeds. Some images have multiple bleeds and could look like [1,0,0,1,0,1]. This was an important consideration in the choice of our cost function. We originally started with categorical cross entropy. This was a mistake that we did not originally catch. Our models ran relatively fine with very good accuracy and very good loss. It’s interesting how the metrics deceived us into believing our model was making good predictions with categorical_cross entropy. It was only when we looked at the probability of individual predictions we found that our model was not at all predicting what we intended. Categorical cross entropy deals with the classification of multi-class classification where there is only one classification. In our case, there could be multiple labels. The first node is classified as whether or not there is a bleed, and the other labels classify whether the bleed is of a specific type. There can be multiple of these other labels, because there can be a multiple of the other bleeds. Categorical cross entropy is clearly wrong in this case. We found this error when creating our Test-Model.py python script. When making individual predictions we found that the predicted label would be [1, 0, 0, .3, 0, 0] or some similar label, but the first node was always 1 - even with no hemorrhaging. Something was clearly off, so we looked more into the ideas behind our cost function and switched over to binary cross entropy. This cost function is useful for binary classification problems where there can be multiple labels. The distinction between these two cost functions was quite simple once realized. Instead of using categorical cross entropy used in multiple-class single-classification, we used the binary cross entropy loss function used in multilabel classification. We are all now very aware of the distinction between binary and categorical cross entropy.



